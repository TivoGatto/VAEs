# -*- coding: utf-8 -*-
"""VAE_Fashon_MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/172WII_f_5dPAvtjtFWP5igDT7SCVF5xn
"""

from google.colab import drive
drive.mount('/content/drive')

# LIBRARIES
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
import tensorflow as tf

import numpy as np
import matplotlib.pyplot as plt

import keras
from keras.datasets import fashion_mnist
from keras.models import Model
from keras.layers import Input, Dense, Conv2D, Conv2DTranspose, Lambda, Reshape, BatchNormalization, ReLU, Flatten
from keras.optimizers import Adam
import keras.backend as K
from keras.models import save_model

# PARAMETERS
input_dim = (28, 28, 1)
intermediate_dim = 256
latent_dim = 128

epochs = 100
batch_size = 100
epsilon_std = 1.0

beta = 0

PATH = './drive/My Drive/Thesis/VAE_models'

# ADDITIONAL FUNCTIONS
def vae_loss(x_true, x_pred):
    x_true = K.reshape(x_true, (-1, np.prod(input_dim)))
    x_pred = K.reshape(x_pred, (-1, np.prod(input_dim)))

    xent_loss = 0.5 * K.sum(K.square(x_true - x_pred), axis=-1)
    reg_loss = 0.5 * K.sum(K.square(z_mean) + K.exp(z_log_var) - 1 - z_log_var, axis=-1)

    return K.mean(xent_loss + reg_loss)

def xent_loss(x_true, x_pred):
    x_true = K.reshape(x_true, (-1, np.prod(input_dim)))
    x_pred = K.reshape(x_pred, (-1, np.prod(input_dim)))

    return K.mean(0.5 * K.sum(K.square(x_true - x_pred), axis=-1))

def reg_loss(x_true, x_pred):
    return K.mean(0.5 * K.sum(K.square(z_mean) + K.exp(z_log_var) - 1 - z_log_var, axis=-1))

def sampling(args):
    z_mean, z_log_var = args
    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=epsilon_std)

    return z_mean + epsilon * K.exp(0.5 * z_log_var)

# MODEL

# ENCODER
x = Input(shape=input_dim)

h = Conv2D(32, 4, strides=(2, 2), padding='same')(x)
h = BatchNormalization()(h)
h = ReLU()(h)

h = Conv2D(64, 4, strides=(2, 2), padding='same')(h)
h = BatchNormalization()(h)
h = ReLU()(h)

shape_before_flattening = K.int_shape(h)[1:]
h = Flatten()(h)

h = Dense(intermediate_dim, activation='relu')(h)

z_mean = Dense(latent_dim)(h)
z_log_var = Dense(latent_dim)(h)

z = Lambda(sampling)([z_mean, z_log_var])

encoder = Model(x, [z, z_mean, z_log_var])

# DECODER
z_in = Input(shape=(latent_dim, ))

h = Dense(np.prod(shape_before_flattening), activation='relu')(z_in)
h = Reshape(shape_before_flattening)(h)

h = Conv2DTranspose(32, 4, strides=(2, 2), padding='same')(h)
h = BatchNormalization()(h)
h = ReLU()(h)

x_recon = Conv2DTranspose(1, 4, strides=(2, 2), padding='same', activation='sigmoid')(h)

decoder = Model(z_in, x_recon)

# VAE
x_decoded = decoder(z)
 
vae = Model(x, x_decoded)
print(vae.summary())
print(decoder.summary())

# DATASET
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')

x_train = np.reshape(x_train, (-1, ) + input_dim)/255
x_test = np.reshape(x_test, (-1, ) + input_dim)/255

print('Train Shape: ', x_train.shape)
print('Test Shape: ', x_test.shape)

# TRAIN THE MODEL
optimizers = Adam(learning_rate=1e-4)
vae.compile(optimizer=optimizers, loss=vae_loss, metrics=[xent_loss, reg_loss])

hist = vae.fit(x_train, x_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)

vae.save('fashion_mnist_Naive_VAE_' + str(latent_dim) + '.h5')
encoder.save('encoder_fashion_mnist_Naive_VAE_' + str(latent_dim) + '.h5')
decoder.save('decoder_fashion_mnist_Naive_VAE_' + str(latent_dim) + '.h5')

plt.style.use('ggplot')

val_loss = hist.history['val_loss']
loss = hist.history['loss']

val_xent_loss = hist.history['val_xent_loss']
xent_loss_hist = hist.history['xent_loss']

val_reg_loss = hist.history['val_reg_loss']
reg_loss_hist = hist.history['reg_loss']

plt.plot(val_loss, 'o-')
plt.plot(loss, 'o-')
plt.legend(['val_loss', 'train_loss'])
plt.title('VAE Loss')
plt.show()

plt.plot(val_xent_loss, 'o-')
plt.plot(xent_loss_hist, 'o-')
plt.legend(['val_xent_loss', 'xent_loss'])
plt.title('|x - x_recon|^2')
plt.show()

plt.plot(val_reg_loss, 'o-')
plt.plot(reg_loss_hist, 'o-')
plt.legend(['val_reg_loss', 'reg_loss'])
plt.title('D_{KL}')
plt.show()

# RECONSTRUCTION
n = 100
digit_size = 28

sample = np.random.choice(np.array(range(len(x_test))), size=n)
x_sample = x_test[sample]

x_recon = vae.predict(x_sample, batch_size=batch_size)

x_sample = np.reshape(x_sample, (-1, 28, 28))
x_recon = np.reshape(x_recon, (-1, 28, 28))

plt.style.use('default')
for i in range(10):
    figure = np.zeros(shape=(digit_size, digit_size * 2))

    figure[:, :digit_size] = x_sample[i]
    figure[:, digit_size:] = x_recon[i] 

    plt.imshow(figure, cmap='gray')
    plt.show()

# GENERATE SAMPLES
n = 10 #figure with n x n digits
digit_size = 28

figure = np.zeros((digit_size * n, digit_size * n))
# we will sample n points randomly sampled

z_sample = np.random.normal(size=(n ** 2, latent_dim), scale=1)
x_decoded = decoder.predict(z_sample)

x_decoded = np.reshape(x_decoded, (-1, 28, 28))
for i in range(n):
    for j in range(n):
        figure[i * digit_size: (i + 1) * digit_size,
            j * digit_size: (j + 1) * digit_size] = x_decoded[i + n * j]

plt.figure(figsize=(10, 10))
plt.imshow(figure, cmap='gray')
plt.show()